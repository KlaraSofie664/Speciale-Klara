{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a0f12c",
   "metadata": {},
   "source": [
    "# Script for Speciale\n",
    "\n",
    "This script indeholde alt kode for modellerne for dette speciale t\n",
    "\n",
    "The script indeholderfølgende:\n",
    "\n",
    "Loading af alle packages, Importing data, Standard logistic regressione, importance Neural network Definding the two neural networks Early stopping Neural network model 1 Neural network model og XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc07a1",
   "metadata": {},
   "source": [
    "### Loading af alle packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb03251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import animation, cm\n",
    "from collections import Counter\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "seed_number = 1000\n",
    "print_flag = 1\n",
    "plot_flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras\n",
    "#pip install --upgrade typing-extensions\n",
    "#pip install tensorflow\n",
    "#pip install --upgrade tensorflow\n",
    "#pip install scikeras\n",
    "#!pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a52ff",
   "metadata": {},
   "source": [
    "## Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d8c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer de nødvendige biblioteker\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importer datasættene\n",
    "df_ordinear = pd.read_csv('/Users/klarafaurholdt/Downloads/snapshot_table_with_active_and_churned_subscriptions.csv', low_memory=False)\n",
    "df_ekstern = pd.read_csv('/Users/klarafaurholdt/Documents/Speciale/ekstern_data.csv', sep=';')\n",
    "\n",
    "# Saml datasættene ved at foretage en venstre sammenføjning på kolonnen 'Month'\n",
    "df = pd.merge(df_ordinear, df_ekstern, on='Month', how='left')\n",
    "\n",
    "# Fjern ubrugte kolonner\n",
    "useless = [\"CarMake\", \"CarModel\", \"CampaignName\", \"PostCode\", \"StartDate\", \"ChurnDate\", \"ChurnPeriod\", \"Year\"]  # Tilpas denne liste efter behov\n",
    "df = df.drop(useless, axis=1)\n",
    "\n",
    "# Opret dummies for kategoriske variabler\n",
    "categorical_columns = [\"ProductType\", \"ProductGroup\", \"CarType\"]  # Tilpas denne liste efter behov\n",
    "df = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Håndter NaN-værdier i kolonnen 'SumKwhHome', \n",
    "#df['SumKwhHome'].fillna(-1, inplace=True)\n",
    "#df['SumKwhNetwork'].fillna(-1, inplace=True)\n",
    "#df['CarType_EV'].fillna(0, inplace=True)\n",
    "#df['ProductType_Clever Box'].fillna(0, inplace=True)\n",
    "\n",
    "# Erstat kommaer med punktummer og konverter til flydende tal\n",
    "df['Elpriser øst øre/kWh'] = df['Elpriser øst øre/kWh'].str.replace(',', '.').astype(float)\n",
    "df['Elpriser vest øre/kWh'] = df['Elpriser vest øre/kWh'].str.replace(',', '.').astype(float)\n",
    "df=df.dropna()\n",
    "\n",
    "# Opdel data i predictors (X) og responsvariabel (y)\n",
    "X = df.drop([\"Churn\", \"Month\"], axis=1)  # Juster responsvariabelnavnet efter behov\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "seed_number = 1000\n",
    "\n",
    "# Opdel data i trænings- og testsæt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_number)\n",
    "\n",
    "# Vis de første rækker af den bearbejdede DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get class weights we use Counter from Collections\n",
    "class_weights = Counter(y)[0]/Counter(y)[1]\n",
    "print('class_weights', class_weights)\n",
    "\n",
    "#Kan gøres endnu større."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1cb66",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded698f3",
   "metadata": {},
   "source": [
    "## Ubalancerede XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5364bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definér XGBoost-modellen\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=seed_number, scale_pos_weight=93)\n",
    "\n",
    "# Træn XGBoost-modellen på træningsdataene\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Forudsig på testdataene\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluér modellen\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Udskriv classification report\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Udskriv forvirringsmatrice\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Forudsig sandsynligheder for begge klasser\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Anvend tærskelværdi på sandsynlighederne for at få endelige forudsigelser\n",
    "y_pred_threshold = (y_pred_proba[:, 1] >= 0.35).astype(int)\n",
    "\n",
    "# Evaluér modellen med tærskelværdi\n",
    "accuracy_threshold = accuracy_score(y_test, y_pred_threshold)\n",
    "print(\"Accuracy with threshold 0.35:\", accuracy_threshold)\n",
    "\n",
    "# Udskriv classification report med tærskelværdi\n",
    "print(\"\\nClassification Report with threshold 0.35:\")\n",
    "print(classification_report(y_test, y_pred_threshold))\n",
    "\n",
    "# Udskriv forvirringsmatrice med tærskelværdi\n",
    "print(\"\\nConfusion Matrix with threshold 0.35:\")\n",
    "print(confusion_matrix(y_test, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b4e43b",
   "metadata": {},
   "source": [
    "## XGBoost balancerede data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5460688",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Antal datapunkter hvor Churn = 1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChurn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_df = df[df['Churn'] == 0]\n",
    "\n",
    "# Antal datapunkter hvor Churn = 1\n",
    "n_samples = len(df[df['Churn'] == 1])\n",
    "\n",
    "# Sample fra filtered_df for at få samme antal datapunkter som Churn = 1\n",
    "sampled_df = filtered_df.sample(n=n_samples)\n",
    "\n",
    "# Find rækkerne hvor Churn = 1\n",
    "churn_1_rows = df[df['Churn'] == 1]\n",
    "\n",
    "# Saml de to datasæt\n",
    "balanced_df = pd.concat([sampled_df, churn_1_rows])\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52481f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beregn gennemsnitslige elpriser pr. kWh, hvis det ikke allerede er gjort\n",
    "balanced_df['AveragePrice_kWh'] = (balanced_df['Elpriser øst øre/kWh'] + balanced_df['Elpriser vest øre/kWh']) / 2\n",
    "\n",
    "# Beregn samlede omkostninger i kroner for hjemmeforbrug og netværksforbrug\n",
    "balanced_df['TotalCostHome_kroner'] = (balanced_df['SumKwhHome'] * balanced_df['AveragePrice_kWh']) / 100\n",
    "balanced_df['TotalCostNetwork_kroner'] = (balanced_df['SumKwhNetwork'] * balanced_df['AveragePrice_kWh']) / 100\n",
    "\n",
    "# Beregn forskellen i 'SumKwhHome' og 'SumKwhNetwork' mellem hver række\n",
    "balanced_df['SumKwhHome_diff'] = balanced_df['SumKwhHome'].diff()\n",
    "balanced_df['SumKwhNetwork_diff'] = balanced_df['SumKwhNetwork'].diff()\n",
    "\n",
    "# Erstat eventuelle NaN-værdier i de nye kolonner med 0\n",
    "balanced_df['SumKwhHome_diff'] = balanced_df['SumKwhHome_diff'].fillna(0)\n",
    "balanced_df['SumKwhNetwork_diff'] = balanced_df['SumKwhNetwork_diff'].fillna(0)\n",
    "\n",
    "# Vise det opdaterede datasæt\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# Opdel data i predictors (X) og responsvariabel (y) for det filtrerede datasæt\n",
    "X_filtered = balanced_df.drop([\"Churn\", \"Month\"], axis=1)\n",
    "y_filtered = balanced_df[\"Churn\"]\n",
    "\n",
    "print('Class weights: ', Counter(y_filtered)[0] / Counter(y_filtered)[1])\n",
    "\n",
    "# Opdel det filtrerede datasæt i trænings- og testsæt\n",
    "X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered,\n",
    "                                                                                        y_filtered,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=seed_number)\n",
    "\n",
    "# Beregn skaleringspositivvægt\n",
    "scale_pos_weight = Counter(y_train_filtered)[0] / Counter(y_train_filtered)[1]\n",
    "\n",
    "# Træn XGBoost-modellen på det filtrerede træningsdata\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                          random_state=seed_number,\n",
    "                          scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Forudsig på det filtrerede testsæt\n",
    "y_pred_filtered = model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluér modellen på det filtrerede testsæt\n",
    "#accuracy_filtered = accuracy_score(y_test_filtered, y_pred_filtered)\n",
    "#print(\"Accuracy (filtered data):\", accuracy_filtered)\n",
    "\n",
    "# Udskriv classification report\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Udskriv forvirringsmatrice\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Forudsig sandsynligheder for begge klasser\n",
    "y_pred_proba = model.predict_proba(X_test_filtered)\n",
    "\n",
    "# Anvend tærskelværdi på sandsynlighederne for at få endelige forudsigelser\n",
    "y_pred_threshold = (y_pred_proba[:, 1] >= 0.35).astype(int)\n",
    "\n",
    "# Evaluér modellen med tærskelværdi\n",
    "accuracy_threshold = accuracy_score(y_test_filtered, y_pred_threshold)\n",
    "print(\"Accuracy with threshold 0.35:\", accuracy_threshold)\n",
    "\n",
    "# Udskriv classification report med tærskelværdi\n",
    "print(\"\\nClassification Report with threshold 0.35:\")\n",
    "print(classification_report(y_test_filtered, y_pred_threshold))\n",
    "\n",
    "# Udskriv forvirringsmatrice med tærskelværdi\n",
    "print(\"\\nConfusion Matrix with threshold 0.35:\")\n",
    "print(confusion_matrix(y_test_filtered, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1764ae1",
   "metadata": {},
   "source": [
    "### Påvirkningsdiagram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50afdca9",
   "metadata": {},
   "source": [
    "## XGBoost  på balancerede data Jan - Apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfb4 = df[(df['Month'] >= 1) & (df['Month'] <= 4) & (df['Churn'] == 0)]\n",
    "\n",
    "n_samplesb4 = len(df[(df['Month'] >= 1) & (df['Month'] <= 4) & (df['Churn'] == 1)])\n",
    "\n",
    "sampled_dfb4 = filtered_dfb4.sample(n=n_samplesb4)\n",
    "\n",
    "churn_1_rowsb4 = df[(df['Month'] >= 1) & (df['Month'] <= 4) & (df['Churn'] == 1)]\n",
    "\n",
    "balanced_dfb4 = pd.concat([sampled_dfb4, churn_1_rowsb4])\n",
    "balanced_dfb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37202a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beregn gennemsnitslige elpriser pr. kWh, hvis det ikke allerede er gjort\n",
    "balanced_dfb4['AveragePrice_kWh'] = (balanced_dfb4['Elpriser øst øre/kWh'] + balanced_dfb4['Elpriser vest øre/kWh']) / 2\n",
    "\n",
    "# Beregn samlede omkostninger i kroner for hjemmeforbrug og netværksforbrug\n",
    "balanced_dfb4['TotalCostHome_kroner'] = (balanced_dfb4['SumKwhHome'] * balanced_dfb4['AveragePrice_kWh']) / 100\n",
    "balanced_dfb4['TotalCostNetwork_kroner'] = (balanced_dfb4['SumKwhNetwork'] * balanced_dfb4['AveragePrice_kWh']) / 100\n",
    "\n",
    "# Beregn forskellen i 'SumKwhHome' og 'SumKwhNetwork' mellem hver række\n",
    "balanced_dfb4['SumKwhHome_diff'] = balanced_dfb4['SumKwhHome'].diff()\n",
    "balanced_dfb4['SumKwhNetwork_diff'] = balanced_dfb4['SumKwhNetwork'].diff()\n",
    "\n",
    "# Erstat eventuelle NaN-værdier i de nye kolonner med 0\n",
    "balanced_dfb4['SumKwhHome_diff'] = balanced_dfb4['SumKwhHome_diff'].fillna(0)\n",
    "balanced_dfb4['SumKwhNetwork_diff'] = balanced_dfb4['SumKwhNetwork_diff'].fillna(0)\n",
    "\n",
    "# Vise det opdaterede datasæt\n",
    "balanced_dfb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdel data i predictors (X) og responsvariabel (y) for det filtrerede datasæt\n",
    "X_filtered = balanced_dfb4.drop([\"Churn\", \"Month\"], axis=1)\n",
    "y_filtered = balanced_dfb4[\"Churn\"]\n",
    "\n",
    "print('Class weights: ', Counter(y_filtered)[0] / Counter(y_filtered)[1])\n",
    "\n",
    "# Opdel det filtrerede datasæt i trænings- og testsæt\n",
    "X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered,\n",
    "                                                                                        y_filtered,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=seed_number)\n",
    "\n",
    "# Beregn skaleringspositivvægt\n",
    "scale_pos_weight = Counter(y_train_filtered)[0] / Counter(y_train_filtered)[1]\n",
    "\n",
    "# Træn XGBoost-modellen på det filtrerede træningsdata\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                          random_state=seed_number,\n",
    "                          scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Forudsig på det filtrerede testsæt\n",
    "y_pred_filtered = model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluér modellen på det filtrerede testsæt\n",
    "accuracy_filtered = accuracy_score(y_test_filtered, y_pred_filtered)\n",
    "#print(\"Accuracy (filtered data):\", accuracy_filtered)\n",
    "\n",
    "# Udskriv classification report\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Udskriv forvirringsmatrice\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Forudsig sandsynligheder for begge klasser\n",
    "y_pred_proba = model.predict_proba(X_test_filtered)\n",
    "\n",
    "# Anvend tærskelværdi på sandsynlighederne for at få endelige forudsigelser\n",
    "y_pred_threshold = (y_pred_proba[:, 1] >= 0.35).astype(int)\n",
    "\n",
    "# Evaluér modellen med tærskelværdi\n",
    "accuracy_threshold = accuracy_score(y_test_filtered, y_pred_threshold)\n",
    "print(\"Accuracy with threshold 0.35:\", accuracy_threshold)\n",
    "\n",
    "# Udskriv classification report med tærskelværdi\n",
    "print(\"\\nClassification Report with threshold 0.35:\")\n",
    "print(classification_report(y_test_filtered, y_pred_threshold))\n",
    "\n",
    "# Udskriv forvirringsmatrice med tærskelværdi\n",
    "print(\"\\nConfusion Matrix with threshold 0.35:\")\n",
    "print(confusion_matrix(y_test_filtered, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f596128",
   "metadata": {},
   "source": [
    "## XGBoost på balancerede data maj-dec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfa5 = df[(df['Month'] >= 5) & (df['Month'] <= 12) & (df['Churn'] == 0)]\n",
    "\n",
    "n_samplesa5 = len(df[(df['Month'] >= 5) & (df['Month'] <= 12) & (df['Churn'] == 1)])\n",
    "\n",
    "sampled_dfa5 = filtered_dfa5.sample(n=n_samplesa5)\n",
    "\n",
    "churn_1_rowsa5 = df[(df['Month'] >= 5) & (df['Month'] <= 12) & (df['Churn'] == 1)]\n",
    "\n",
    "balanced_dfa5 = pd.concat([sampled_dfa5, churn_1_rowsa5])\n",
    "balanced_dfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beregn gennemsnitslige elpriser pr. kWh, hvis det ikke allerede er gjort\n",
    "balanced_dfa5['AveragePrice_kWh'] = (balanced_dfa5['Elpriser øst øre/kWh'] + balanced_dfa5['Elpriser vest øre/kWh']) / 2\n",
    "\n",
    "# Beregn samlede omkostninger i kroner for hjemmeforbrug og netværksforbrug\n",
    "balanced_dfa5['TotalCostHome_kroner'] = (balanced_dfa5['SumKwhHome'] * balanced_dfa5['AveragePrice_kWh']) / 100\n",
    "balanced_dfa5['TotalCostNetwork_kroner'] = (balanced_dfa5['SumKwhNetwork'] * balanced_dfa5['AveragePrice_kWh']) / 100\n",
    "\n",
    "# Beregn forskellen i 'SumKwhHome' og 'SumKwhNetwork' mellem hver række\n",
    "balanced_dfa5['SumKwhHome_diff'] = balanced_dfa5['SumKwhHome'].diff()\n",
    "balanced_dfa5['SumKwhNetwork_diff'] = balanced_dfa5['SumKwhNetwork'].diff()\n",
    "\n",
    "# Erstat eventuelle NaN-værdier i de nye kolonner med 0\n",
    "balanced_dfa5['SumKwhHome_diff'] = balanced_dfa5['SumKwhHome_diff'].fillna(0)\n",
    "balanced_dfa5['SumKwhNetwork_diff'] = balanced_dfa5['SumKwhNetwork_diff'].fillna(0)\n",
    "\n",
    "# Vise det opdaterede datasæt\n",
    "balanced_dfa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opdel data i predictors (X) og responsvariabel (y) for det filtrerede datasæt\n",
    "X_filtered = balanced_dfa5.drop([\"Churn\", \"Month\"], axis=1)\n",
    "y_filtered = balanced_dfa5[\"Churn\"]\n",
    "\n",
    "print('Class weights: ', Counter(y_filtered)[0] / Counter(y_filtered)[1])\n",
    "\n",
    "# Opdel det filtrerede datasæt i trænings- og testsæt\n",
    "X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered,\n",
    "                                                                                        y_filtered,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=seed_number)\n",
    "\n",
    "# Beregn skaleringspositivvægt\n",
    "scale_pos_weight = Counter(y_train_filtered)[0] / Counter(y_train_filtered)[1]\n",
    "\n",
    "# Træn XGBoost-modellen på det filtrerede træningsdata\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                          random_state=seed_number,\n",
    "                          scale_pos_weight=scale_pos_weight)\n",
    "model.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Forudsig på det filtrerede testsæt\n",
    "y_pred_filtered = model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluér modellen på det filtrerede testsæt\n",
    "accuracy_filtered = accuracy_score(y_test_filtered, y_pred_filtered)\n",
    "#print(\"Accuracy (filtered data):\", accuracy_filtered)\n",
    "\n",
    "# Udskriv classification report\n",
    "#print(\"\\nClassification Report:\")\n",
    "#print(classification_report(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Udskriv forvirringsmatrice\n",
    "#print(\"\\nConfusion Matrix:\")\n",
    "#print(confusion_matrix(y_test_filtered, y_pred_filtered))\n",
    "\n",
    "# Forudsig sandsynligheder for begge klasser\n",
    "y_pred_proba = model.predict_proba(X_test_filtered)\n",
    "\n",
    "# Anvend tærskelværdi på sandsynlighederne for at få endelige forudsigelser\n",
    "y_pred_threshold = (y_pred_proba[:, 1] >= 0.35).astype(int)\n",
    "\n",
    "# Evaluér modellen med tærskelværdi\n",
    "accuracy_threshold = accuracy_score(y_test_filtered, y_pred_threshold)\n",
    "print(\"Accuracy with threshold 0.35:\", accuracy_threshold)\n",
    "\n",
    "# Udskriv classification report med tærskelværdi\n",
    "print(\"\\nClassification Report with threshold 0.35:\")\n",
    "print(classification_report(y_test_filtered, y_pred_threshold))\n",
    "\n",
    "# Udskriv forvirringsmatrice med tærskelværdi\n",
    "print(\"\\nConfusion Matrix with threshold 0.35:\")\n",
    "print(confusion_matrix(y_test_filtered, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50ac16",
   "metadata": {},
   "source": [
    "# Standard Logistisk Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train\n",
    "# drop columns with num unique = 1\n",
    "\n",
    "# For regression models, faster convergence can be obtained by applying a standard scaling and normalization.\n",
    "# This assigns equal weight to features, even if one columns has values on the order of 000's and one has small decimals.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3111c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a standard logistic regression model\n",
    "LR_stand = LogisticRegression(\n",
    "    class_weight = {\n",
    "        0 : 1,\n",
    "        1 : 50\n",
    "    }\n",
    ")\n",
    "\n",
    "# Gridsearch is used to find the optimal hyperparameters\n",
    "parameters = {\n",
    "    'solver': ['saga'],#, 'lbfgs', 'sag' and 'newton-cg'],\n",
    "    'penalty': ['none']\n",
    "}\n",
    "LR_stand_Grid = GridSearchCV(LR_stand, parameters, cv=5, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "# Training the standard logistic model\n",
    "LR_stand_Grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Printing the optimal hyperparameters\n",
    "print(LR_stand_Grid.best_estimator_)\n",
    "\n",
    "# Model prediction\n",
    "LR_stand_preds = LR_stand_Grid.predict(X_test_scaled)\n",
    "\n",
    "# Probability predicting for ROC plot\n",
    "LR_stand_probs = LR_stand_Grid.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "# Model prediction with threshold\n",
    "LR_stand_preds = np.where(LR_stand_probs > 0.5, 1, 0)\n",
    "\n",
    "# Creating confusion matrix\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = LR_stand_preds\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_prediction['status'], y_prediction[\"Preds\"], rownames=['Actuals'], colnames=['Predicted'])\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(LR_stand_preds, y_test ,output_dict = True)\n",
    "print(\"Performance measurements: \\n\", pd.DataFrame(performance_measurements))\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, LR_stand_probs)\n",
    "\n",
    "# Calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', label='Baseline', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "# Gem diagrammet som en fil\n",
    "plt.savefig('ROC_curve.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"Standard logistic regression is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce81bf",
   "metadata": {},
   "source": [
    "# Neuralt Netværk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e104f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines neural network model 1\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(4, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Defines neural network model 2\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(20, input_dim=X_train.shape[1], activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the two models\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ModelCheckpoint callbacks with updated file extensions\n",
    "mc1 = tf.keras.callbacks.ModelCheckpoint('model1.keras', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "mc2 = tf.keras.callbacks.ModelCheckpoint('model2.keras', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "  \n",
    "# Transposes the y_train set\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "# Defines early stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "# Make early stopping for neural network model 1\n",
    "EarlyStopping_model_1 = model1.fit(X_train, y_train, epochs=100, batch_size=10, callbacks=[es, mc1], validation_split=0.2)\n",
    "\n",
    "# Print the training loss and validation loss plot for neural network model 1\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.plot(EarlyStopping_model_1.history['loss'], label='train')\n",
    "plt.plot(EarlyStopping_model_1.history['val_loss'], label='validation error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make early stopping for neural network model 2\n",
    "EarlyStopping_model_2 = model2.fit(X_train, y_train, epochs=100, batch_size=10, callbacks=[es, mc2], validation_split=0.2)\n",
    "\n",
    "# Print the training loss and validation loss plot for neural network model 2\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.plot(EarlyStopping_model_2.history['loss'], label='train')\n",
    "plt.plot(EarlyStopping_model_2.history['val_loss'], label='validation error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the models using a function for each\n",
    "def create_NN_model1():\n",
    "    NN_model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model1\n",
    "\n",
    "def create_NN_model2():\n",
    "    NN_model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model2\n",
    "\n",
    "# Initialize and train Model 1 and Model 2\n",
    "model1 = create_NN_model1()\n",
    "history1 = model1.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "model2 = create_NN_model2()\n",
    "history2 = model2.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    probs = model.predict(X_test).flatten()\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions, zero_division=1))\n",
    "    return probs\n",
    "\n",
    "probs1 = evaluate_model(model1, X_test, y_test)\n",
    "probs2 = evaluate_model(model2, X_test, y_test)  # Corrected function name here\n",
    "\n",
    "# Function to plot training history including a threshold line at 0.5\n",
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC')\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold at 0.5')\n",
    "    plt.title(f'Training and Validation AUC for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history for both models\n",
    "plot_training_history(history1, \"Model 1\")\n",
    "plot_training_history(history2, \"Model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e194938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the models using a function for each\n",
    "def create_NN_model1():\n",
    "    NN_model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model1\n",
    "\n",
    "def create_NN_model2():\n",
    "    NN_model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model2\n",
    "\n",
    "# Initialize and train Model 1 and Model 2\n",
    "model1 = create_NN_model1()\n",
    "history1 = model1.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "model2 = create_NN_model2()\n",
    "history2 = model2.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    probs = model.predict(X_test).flatten()\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions, zero_division=1))\n",
    "    return probs\n",
    "\n",
    "probs1 = evaluate_model(model1, X_test, y_test)\n",
    "probs2 = evaluate_model(model2, X_test, y_test)  # Corrected function name here\n",
    "\n",
    "# Function to plot training history including a threshold line at 0.5\n",
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC')\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold at 0.5')\n",
    "    plt.title(f'Training and Validation AUC for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history for both models\n",
    "plot_training_history(history1, \"Model 1\")\n",
    "plot_training_history(history2, \"Model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Assuming the keys are known from the print statements\n",
    "    auc_key = next(key for key in history.history.keys() if 'auc' in key and not 'val_' in key)\n",
    "    val_auc_key = next(key for key in history.history.keys() if 'val_auc' in key)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history[auc_key], label='Training AUC')\n",
    "    plt.plot(epochs, history.history[val_auc_key], label='Validation AUC')\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold at 0.5')\n",
    "    plt.title(f'Training and Validation AUC for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the models using a function for each\n",
    "def create_NN_model1():\n",
    "    NN_model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model1\n",
    "\n",
    "def create_NN_model2():\n",
    "    NN_model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Using Input layer\n",
    "        tf.keras.\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(20, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model2\n",
    "\n",
    "# Initialize and train Model 1 and Model 2\n",
    "model1 = create_NN_model1()\n",
    "history1 = model1.fit(X_train, y_train, epochs=100, batch_size=40, verbose=1, validation_split=0.2)\n",
    "\n",
    "model2 = create_NN_model2()\n",
    "history2 = model2.fit(X_train, y_train, epochs=100, batch_size=40, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    probs = model.predict(X_test).flatten()\n",
    "    predictions = (probs > 0.35).astype(int)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions, zero_division=1))\n",
    "    return probs\n",
    "\n",
    "probs1 = evaluate_model(model1, X_test, y_test)\n",
    "probs2 = evaluate_model(model2, X_test, y_test)  # Corrected function name here\n",
    "\n",
    "# Function to plot training history including a threshold line at 0.5\n",
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['auc'], label='Training AUC')\n",
    "    plt.plot(epochs, history.history['val_auc'], label='Validation AUC')\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', label='Threshold at 0.35')\n",
    "    plt.title(f'Training and Validation AUC for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history for both models\n",
    "plot_training_history(history1, \"Model 1\")\n",
    "plot_training_history(history2, \"Model 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eeecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Assuming the keys are known from the print statements\n",
    "    auc_key = next(key for key in history.history.keys() if 'auc' in key and not 'val_' in key)\n",
    "    val_auc_key = next(key for key in history.history.keys() if 'val_auc' in key)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history[auc_key], label='Training AUC')\n",
    "    plt.plot(epochs, history.history[val_auc_key], label='Validation AUC')\n",
    "    plt.axhline(y=0.35, color='red', linestyle='--', label='Threshold at 0.35')\n",
    "    plt.title(f'Training and Validation AUC for {model_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history1, \"Model 1\")\n",
    "plot_training_history(history2, \"Model 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
